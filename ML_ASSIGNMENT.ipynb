{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Preprocessing"
      ],
      "metadata": {
        "id": "SBGXZce1yQ_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing the Libraries and  Dataset"
      ],
      "metadata": {
        "id": "3I-QJk1zwDbQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhDzamiYv0Y7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/MLE-Assignment.csv')"
      ],
      "metadata": {
        "id": "rExz7-WHwTJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Basic Info and Overview of the Dataset\n"
      ],
      "metadata": {
        "id": "Rru4U2-MxBUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Viewing First 5 Rows : \")\n",
        "data.head(5)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a0X3cFQ9weJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Basic Dataset Info : \")\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "YvYo3BXZw5SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Checking Data Distribution  "
      ],
      "metadata": {
        "id": "TQD3uchOqfJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Dynamically calculate number of rows & columns\n",
        "num_plots = len(numeric_cols)\n",
        "num_cols = 3\n",
        "num_rows = int(np.ceil(num_plots / num_cols))\n",
        "\n",
        "plt.figure(figsize=(12, 4 * num_rows))\n",
        "for i, col in enumerate(numeric_cols, 1):\n",
        "    plt.subplot(num_rows, num_cols, i)\n",
        "    sns.histplot(data[col], kde=True, bins=30)\n",
        "    plt.title(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LjCdhiA9qnO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Checking Missing Values in the Dataset"
      ],
      "metadata": {
        "id": "P9jhseN2yCo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Checking Missing Values :\")\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "LQCvtDP0xnb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace([\"\", \" \", \"NaN\", \"NULL\"], np.nan, inplace=True)\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "nzBjCJKVxvYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Checking and Removing Duplicates"
      ],
      "metadata": {
        "id": "LFquJ0wA29RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = data.duplicated().sum()\n",
        "print(f\"Number of Duplicates Present in the Dataset is : \", {duplicates})"
      ],
      "metadata": {
        "id": "hKYBfmxBzMCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Outlier Detection"
      ],
      "metadata": {
        "id": "LOhmp4Smq4EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Dynamically determine rows and columns\n",
        "num_plots = len(numeric_cols)\n",
        "num_cols = 3  # Keep columns fixed at 3\n",
        "num_rows = int(np.ceil(num_plots / num_cols))\n",
        "\n",
        "plt.figure(figsize=(12, 4 * num_rows))\n",
        "for i, col in enumerate(numeric_cols, 1):\n",
        "    plt.subplot(num_rows, num_cols, i)\n",
        "    sns.boxplot(y=data[col])\n",
        "    plt.title(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GgjRr5vgrC17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data.select_dtypes(include=['number'])\n",
        "\n",
        "# Compute IQR\n",
        "Q1 = numeric_data.quantile(0.25)\n",
        "Q3 = numeric_data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers = ((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR)))\n",
        "outlier_counts = outliers.sum()\n",
        "print(\"Outlier count per column:\\n\", outlier_counts)"
      ],
      "metadata": {
        "id": "MLA0OdsWzr5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "for col in numeric_data.columns:\n",
        "    data[col] = winsorize(data[col], limits=[0.05, 0.05])  # Capping at 5% on both sides\n"
      ],
      "metadata": {
        "id": "FymLchufLTme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Outlier Detection in DON Concentration"
      ],
      "metadata": {
        "id": "ptFIn_dt3_25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=data['vomitoxin_ppb'], color=\"red\")\n",
        "plt.title(\"Outlier Detection in DON Concentration\")\n",
        "plt.xlabel(\"DON Concentration (ppb)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2V-gNs-3FMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Analyzing Outliers Impact\n",
        "If the target variable is heavily skewed, then applying log transformation instead of removing outliers"
      ],
      "metadata": {
        "id": "Qr6iJ09Z73VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking The Distribution of Target Variable\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data['vomitoxin_ppb'], bins=30, kde=True)\n",
        "plt.xlabel('DON Concentraion(ppd)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of DON Concentration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_knHyIbQ4qZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate skewness\n",
        "from scipy.stats import skew\n",
        "skewness = skew(data['vomitoxin_ppb'])\n",
        "print(f\"Skewness of DON Concentration : {skewness:.2f}\")"
      ],
      "metadata": {
        "id": "YMd4AB0k85Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if skewness is high\n",
        "if abs(skewness) > 1:\n",
        "    print(\"The data is highly skewed. A log transformation might help.\")\n",
        "elif abs(skewness) > 0.5:\n",
        "    print(\"The data is moderately skewed.\")\n",
        "else:\n",
        "    print(\"The data is approximately normal.\")"
      ],
      "metadata": {
        "id": "UNkPlKxA9ye1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Applying Log Transformation\n",
        "For reducing skewness while preserving important variations"
      ],
      "metadata": {
        "id": "PxjDWhfc_F4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['log_vomitoxin_ppd'] = np.log1p(data['vomitoxin_ppb'])"
      ],
      "metadata": {
        "id": "QzytdHth96As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting histogram after log transformation\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(data['log_vomitoxin_ppd'], bins=30, kde=True)\n",
        "plt.xlabel('Log Transformed DON Concentration(ppd)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Log Transformed DON Concentration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tz3LbhUK_lL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_skewness = skew(data['log_vomitoxin_ppd'])\n",
        "print(f\"Skewness After Log Transformation : {new_skewness:.2f}\")"
      ],
      "metadata": {
        "id": "5Rd6SUuLABPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if skewness is high\n",
        "if abs(new_skewness) > 1:\n",
        "    print(\"The data is highly skewed. A log transformation might help.\")\n",
        "elif abs(new_skewness) > 0.5:\n",
        "    print(\"The data is moderately skewed.\")\n",
        "else:\n",
        "    print(\"The data is approximately normal.\")"
      ],
      "metadata": {
        "id": "pZN6pnZt07Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Standardizing or Normalizing the Spectral Data."
      ],
      "metadata": {
        "id": "s97I1c8QNkyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "numeric_data = data.select_dtypes(include=['number'])\n",
        "\n",
        "# Apply scaling only to numeric columns\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(data[numeric_cols]), columns=numeric_cols)\n",
        "\n",
        "# Keep non-numeric columns and merge back\n",
        "data_scaled = pd.concat([data.drop(columns=numeric_cols), scaled_data], axis=1)\n",
        "\n",
        "print(\"Scaling applied successfully!\")"
      ],
      "metadata": {
        "id": "lZKswtjWL8VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_spectral_columns = [\"hsi_id\", \"vomitoxin_ppb\"]\n",
        "spectral_columns = [col for col in data.columns if col not in non_spectral_columns]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data[spectral_columns] = scaler.fit_transform(data[spectral_columns])\n"
      ],
      "metadata": {
        "id": "reqIN8MoDgcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Generating Visualizations"
      ],
      "metadata": {
        "id": "8eriqbTX8TWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.1 Line Plot for Average Reflectance Over Wavelengths"
      ],
      "metadata": {
        "id": "N_44HI1DN5-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "avg_reflectance = data[spectral_columns].mean()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(avg_reflectance.index, avg_reflectance.values)\n",
        "plt.xlabel(\"Wavelength\")\n",
        "plt.ylabel(\"Average Reflectance\")\n",
        "plt.title(\"Average Reflectance Over Wavelengths\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wYWN8N2n8PCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.2 Heatmap for Correlation Analysis"
      ],
      "metadata": {
        "id": "DNO173ykODbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(data[spectral_columns].corr(), cmap=\"coolwarm\", annot=False)\n",
        "plt.title(\"Heatmap of Spectral Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gUYdewVs8a8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.3 Pairplot for Sample Comparisons"
      ],
      "metadata": {
        "id": "nHn193nROJRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data[spectral_columns[:5]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YNyLtxPx8fBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.4 Line Plot for Reflectance Over Wavelengths"
      ],
      "metadata": {
        "id": "3X9OHsTEOOb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming columns '0' to '447' are spectral bands\n",
        "spectral_columns = [col for col in data.columns if col.isdigit()]\n",
        "\n",
        "# Compute average reflectance across samples\n",
        "avg_reflectance = data[spectral_columns].mean()\n",
        "\n",
        "# Plot reflectance trend\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(avg_reflectance.index, avg_reflectance.values, marker='o', linestyle='-', label=\"Avg Reflectance\")\n",
        "plt.xlabel(\"Wavelength Index\")\n",
        "plt.ylabel(\"Average Reflectance\")\n",
        "plt.title(\"Average Reflectance Over Wavelengths\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KzatHmsf8jZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Advanced  Data Quality Checks"
      ],
      "metadata": {
        "id": "83COgSjxOdZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.1 Automated Sensor Drift & Data Consistency Checks"
      ],
      "metadata": {
        "id": "pfY9_fmSOnZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# Compute Reflectance Differences (First-Order Derivative Check)\n",
        "reflectance_diff = data[spectral_columns].diff(axis=1).mean(axis=1)\n",
        "\n",
        "# Handle NaN values if they exist\n",
        "reflectance_diff.fillna(0, inplace=True)\n",
        "\n",
        "# Efficiently join the column to avoid fragmentation issues\n",
        "data = pd.concat([data, reflectance_diff.rename(\"reflectance_diff\")], axis=1).copy()\n",
        "\n",
        "# Confirm NaN values are handled\n",
        "print(\"NaN count in reflectance_diff:\", data[\"reflectance_diff\"].isna().sum())\n",
        "print(data[\"reflectance_diff\"].describe())\n",
        "\n",
        "# 🔹 Visualize Reflectance Change Over Wavelengths\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data.index, data[\"reflectance_diff\"], label=\"Reflectance Change Rate\", linestyle=\"dashed\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Reflectance Change\")\n",
        "plt.title(\"Sensor Drift Analysis\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SLgFCLT7GaK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2 Feature Engineering: Spectral Indices (NDVI, NDWI, MSI)"
      ],
      "metadata": {
        "id": "QEViszB9Oz_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure required bands exist in dataset\n",
        "if all(col in data.columns for col in [\"70\", \"100\", \"140\"]):\n",
        "    # Compute spectral indices\n",
        "    data[\"NDVI\"] = (data[\"100\"] - data[\"70\"]) / (data[\"100\"] + data[\"70\"])\n",
        "    data[\"NDWI\"] = (data[\"100\"] - data[\"140\"]) / (data[\"100\"] + data[\"140\"])\n",
        "    data[\"MSI\"] = data[\"140\"] / data[\"100\"]\n",
        "\n",
        "    print(\" Spectral Indices (NDVI, NDWI, MSI) added successfully!\")\n",
        "\n",
        "    # Heatmap for Feature Correlation Analysis\n",
        "    import seaborn as sns\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(data[[\"NDVI\", \"NDWI\", \"MSI\", \"vomitoxin_ppb\"]].corr(), annot=True, cmap=\"coolwarm\")\n",
        "    plt.title(\"Correlation of Spectral Indices with Target Variable\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\" Required spectral bands not found. Skipping spectral indices computation.\")\n"
      ],
      "metadata": {
        "id": "OFjZasWVGgnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Preprocessed Data"
      ],
      "metadata": {
        "id": "Q3gA24YkI48v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"preprocessed_data.csv\", index=False)\n",
        "\n",
        "print(\"Preprocessed data saved successfully!\")"
      ],
      "metadata": {
        "id": "5v8GlLeqGlDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Logging to Data Preprocessing"
      ],
      "metadata": {
        "id": "xikeiz27Mjn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Logging data shape and missing values\n",
        "logging.info(f\"Dataset shape after preprocessing: {data.shape}\")\n",
        "logging.info(f\"Missing values after preprocessing:\\n{data.isnull().sum()}\")\n",
        "\n",
        "# Log feature scaling or encoding\n",
        "logging.info(\"Feature scaling and encoding completed.\")\n"
      ],
      "metadata": {
        "id": "plHDXc3vMuk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Unit Tests"
      ],
      "metadata": {
        "id": "Fuyl1cXHM92Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "def preprocess_data(df):\n",
        "    if data.isnull().sum().sum() > 0:\n",
        "        logging.warning(\"Dataset contains missing values!\")\n",
        "    return df.fillna(0)\n",
        "\n",
        "# Define unit tests\n",
        "class TestPreprocessing(unittest.TestCase):\n",
        "    def test_missing_values(self):\n",
        "        df_test = pd.DataFrame({\"A\": [1, 2, np.nan], \"B\": [3, np.nan, 5]})\n",
        "        df_result = preprocess_data(df_test)\n",
        "        self.assertFalse(df_result.isnull().values.any())\n",
        "\n",
        "# Run tests\n",
        "unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "id": "NWEjjYaeNBD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "KykI4sL3Vjfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installing required Libraries"
      ],
      "metadata": {
        "id": "bYeIHikxVsOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn tensorflow optuna\n"
      ],
      "metadata": {
        "id": "abiFMFMlJCJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import optuna"
      ],
      "metadata": {
        "id": "ySm-5Bf5WyVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load Preprocessed Data"
      ],
      "metadata": {
        "id": "kn2l7LY8V4Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/preprocessed_data.csv')"
      ],
      "metadata": {
        "id": "dFr4pbtAVwbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Defining Features X and Target Variable Y"
      ],
      "metadata": {
        "id": "QbMN8LWBWIyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop(columns=['hsi_id','vomitoxin_ppb'])\n",
        "y = df[\"vomitoxin_ppb\"]"
      ],
      "metadata": {
        "id": "OrlQWp2hWHJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering (Mutual Information)"
      ],
      "metadata": {
        "id": "2VrupHdPPh5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
        "\n",
        "mi_scores = mutual_info_regression(x, y)\n",
        "mi_sorted = sorted(zip(x.columns, mi_scores), key=lambda x: x[1], reverse=True)\n",
        "selected_mi_features = [feature for feature, score in mi_sorted[:10]]"
      ],
      "metadata": {
        "id": "3MyvOpYPPmB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Feature Selection (ANOVA F-test)\n",
        "f_scores, p_values = f_regression(x, y)\n",
        "selected_f_features = x.columns[p_values < 0.05]"
      ],
      "metadata": {
        "id": "R2WwUEMIPpDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Common Features from Both Methods\n",
        "selected_features = list(set(selected_mi_features) & set(selected_f_features))"
      ],
      "metadata": {
        "id": "wucA1DplPrfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the Dataset with Selected Features\n",
        "X_selected = x[selected_features]"
      ],
      "metadata": {
        "id": "vZ79jK7fPumN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Splitting Data into Training and Testing Variable"
      ],
      "metadata": {
        "id": "p6eNmhS2WhYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tgsdYTqfWdbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Normalizing the Features"
      ],
      "metadata": {
        "id": "YCh6eLQOPaxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Normalize Features\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "ncp-7kJ7nxGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. XG Boost Model"
      ],
      "metadata": {
        "id": "qImQTm2HqBVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "CgpzO1R_qRS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train XGBoost Model\n",
        "model = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "gjEl4vgxqJl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Make Predictions\n",
        "y_pred = model.predict(x_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "XtKngqaxqNs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "OQp5Aa_qPl7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Calculate regression metrics"
      ],
      "metadata": {
        "id": "2PKcd9p9P4r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Evaluate Model Performance\n",
        "from sklearn.metrics import r2_score\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "lzQMDpDpqwyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" MAE: {mae:.4f}, MSE: {mse:.4f}, R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "edyhCyjPq063"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Visual Evaluation"
      ],
      "metadata": {
        "id": "97K1msFZFN4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Plotting a scatter plot comparing actual vs. predicted values"
      ],
      "metadata": {
        "id": "SRJv6kfkFcvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Scatter Plot: Actual vs. Predicted\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel(\"Actual Vomitoxin_ppb\")\n",
        "plt.ylabel(\"Predicted Vomitoxin_ppb\")\n",
        "plt.title(\"Actual vs. Predicted Values (XGBoost)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L53Q17nurApl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Performing Residual Analysis to identify any systematic errors."
      ],
      "metadata": {
        "id": "MrQyj9pPFUOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract actual values (vomitoxin_ppd column)\n",
        "actual_values = df[\"vomitoxin_ppb\"].values\n",
        "\n",
        "# Ensure actual & predicted values have the same length\n",
        "actual_values = actual_values[:len(y_pred)]\n",
        "\n",
        "# Create a DataFrame to store actual vs. predicted values\n",
        "results_df = pd.DataFrame({\"Actual\": actual_values, \"Predicted\": y_pred})\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = \"actual_vs_predicted.csv\"\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"CSV file created: {csv_path}\")"
      ],
      "metadata": {
        "id": "a6xif1FKFkXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_predicted_data = pd.read_csv('/content/actual_vs_predicted.csv')\n",
        "print(actual_predicted_data.head())"
      ],
      "metadata": {
        "id": "74RjyLQTGQhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Residuals\n",
        "residuals = actual_values - y_pred"
      ],
      "metadata": {
        "id": "r-Wco-NKHmxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Residual Distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(residuals, kde=True, bins=30, color=\"purple\")\n",
        "plt.axvline(0, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Residual Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2bwFlQ-IH4mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residuals vs. Actual Values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=actual_values, y=residuals, color='green', alpha=0.6)\n",
        "plt.axhline(0, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.xlabel(\"Actual Vomitoxin_ppb\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Actual Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tbs8YG7JH-_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Interpretablity and Expalinability"
      ],
      "metadata": {
        "id": "WXMUTz3A7YiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Feature Importance"
      ],
      "metadata": {
        "id": "33Ea9p5u7jln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "# Plot feature importance\n",
        "xgb.plot_importance(model)\n",
        "plt.title(\"Feature Importance (XGBoost)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HE8V5OUbrD4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Expalin Model Predictions With SHAP"
      ],
      "metadata": {
        "id": "xElcYPrn7vCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "# Initialize SHAP explainer\n",
        "explainer = shap.Explainer(model, x_train)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer(x_test)\n",
        "\n",
        "# Summary plot - Shows feature importance globally\n",
        "shap.summary_plot(shap_values, x_test)\n"
      ],
      "metadata": {
        "id": "XGHYy7Do7obL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Local Interpretability with LIME"
      ],
      "metadata": {
        "id": "GiIDCPl28IsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Install & Import LIME"
      ],
      "metadata": {
        "id": "sN0HB60_8L9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ],
      "metadata": {
        "id": "o1hcufSD76TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create LIME Explainer"
      ],
      "metadata": {
        "id": "WeulvxRp8Uok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(x_train, np.ndarray):\n",
        "    x_train = pd.DataFrame(x_train, columns=[f\"Feature_{i}\" for i in range(x_train.shape[1])])\n",
        "\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=np.array(x_train),\n",
        "    feature_names=x_train.columns,\n",
        "    mode=\"regression\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "sgVs7qjR8RlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a random sample"
      ],
      "metadata": {
        "id": "PJvhKYgD8hWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(x_test, np.ndarray):\n",
        "    x_test = pd.DataFrame(x_test, columns=[f\"Feature_{i}\" for i in range(x_test.shape[1])])\n",
        "\n",
        "i = 10  # Index of the sample to explain\n",
        "exp = explainer.explain_instance(x_test.iloc[i], model.predict)\n",
        "\n",
        "# Show explanation\n",
        "exp.show_in_notebook()"
      ],
      "metadata": {
        "id": "fKg7xjve8Yqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Trained Model"
      ],
      "metadata": {
        "id": "Q5aKO-ydOScw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, \"spectral_model.pkl\")\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "Ch7JCXBk85lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}